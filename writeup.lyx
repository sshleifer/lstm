#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
LSTM with different Encodings
\end_layout

\begin_layout Subsubsection*
Abstract
\end_layout

\begin_layout Standard
We wrote an LSTM implementation that parametrizes the number of characters
 that each input `x` encodes – the $n$ in $n-gram$ – and measured training
 efficiency and the perplexity of text generated by a model trained on 1
 billion characters from Wikipedia.
 Results indicate that, even if we embed the one-hot-encoded bigram or trigram
 vector representations in a 128 element vector, training is more efficient
 on unigrams; the text that the model generates after 10 minutes on a MacBook
 Pro is more reasonable when the model is trained on the 27 element one-hot
 representation of unigrams.
\end_layout

\begin_layout Subsubsection*
Motivation for the LSTM Architecture
\end_layout

\begin_layout Standard
At a high level, the sequence modeling problem is to predict 
\begin_inset Formula $x[t+1]$
\end_inset

 given 
\begin_inset Formula $x[0:t]$
\end_inset

.
 Traditional feedforward neural networks are not well suited for this task
 because they require evenly lengthed inputs.
 The original solution to this was recurrent neural networks, which use
 loops (a neurons output feeds back into previous neurons as input) to allow
 information from the past to persist.
 Unfortunately, RNN overwrite the information they are propogating too quickly
 for many tasks.
 For example, an RNN might not remember the subject of a sentence after
 a few words, even though that information is important for predicting the
 sentences' verb.
 If we think of an unrolled RNN as a series of t repeating modules, each
 creating an output h[t] and taking input x[t], then we see that there is
 a lot of back propagation happening between the first module and the Nth
 module.
 LSTM, which are a specific kind of RNN, solve this long-term dependency
 problem by adding parameters to the repeated module so that there is a
 vector, the cell state, dedicated to persisting information through the
 network.
 The cell states are guarded from overwriting by a forget gate.
 
\end_layout

\begin_layout Standard
RNN send their output back While there has been considerable research showing
 that the memory of traditional Recurrent NetexLong Short-Term Memory models
 improve on Recurrent Neural Networks by adding parameters to each module
 that contain information from many steps ago.
 Each module has the ability to add or remove informaton from the Cell state,
 with regulation mediated by sigmoid layers whose outputted probabilities
 indicate how much of the previous hidden state and input x[t] to let into
 the cell state.
\end_layout

\begin_layout Paragraph*
Encoding Tradeoffs
\end_layout

\begin_layout Standard
The size of the vector needed to represent the one-hot-encoding of each
 element in the sequence is 27 ascii characters ^n, where n is the number
 of characters represented by each x.
 When these vectors get long, practitioners tend to embed them in a lower
 dimensional space.
 In our experiment, we one-hot-encode the n-grams and then embed them in
 a 128 element vector.
 So, for bigrams and trigrams, we are forced to learn an embedding and that
 embedding is lossy, two major disadvantages of moving past unigrams.
 The benefit of training on larger n-grams is that the full x sequences
 are shorter, so it may be easier to encode a dependency that is 10 characters
 away if that is only 5 modules away.
 To address this we also vary how far backwards errors can back-propagate.
\end_layout

\begin_layout Subsubsection*
Results
\end_layout

\begin_layout Standard
Training on trigrams is very slow and doesn't work very well.
 In the two experiments we ran, we let a trigram model run for 1000 steps,
 first with 5 unrollings and then with 10 unrollings.
 The 5 unrollings model took 8 minutes compared with 15 and scored roughly
 the same perplexity on the 1000 character validation set, but the text
 generated is not english.
 The major problem seems to be that the words end up being very long.
\end_layout

\begin_layout Standard
Bigram models were more succesful, largely because they ran 7x faster than
 trigrams per batch.
 The difference here might be the time and loss associated with training
 the emedding.
\end_layout

\begin_layout Quotation
mkyekkmal fut tthiuhjor ovqtwgt oenttordborenbzuarbwaq hacenadwageivgtheforhwd
 hih t thonawix uz thsgsolizxajecthikmrvedwarctr maas aaaconpujaursidhe
 istfbytio ane nvennumguasfghellutof s dntrpubic sr t oy atwo deothen o
 zem ionype imonorgu - Trigram RNN trained for 15 minutes.
\end_layout

\begin_layout Subsubsection*
Next Steps
\end_layout

\begin_layout Enumerate
Modify WordRNN to generate text
\end_layout

\begin_layout Enumerate
Experiment with using the unigram model to measure the perplexity of other
 models' generated text, for easier comparisons.
\end_layout

\begin_layout Enumerate
Allow more time for training.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
key "LSTM BlogPost"

\end_inset

<table>
\end_layout

\end_body
\end_document
